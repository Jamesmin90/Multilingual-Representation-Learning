{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 10: Multilingual Representation Learning\n",
    "\n",
    "In this exercise, you will induce and explore multilingual text representations, cross-lingual word embeddings and multilingual encoders.\n",
    "\n",
    "You should complete the parts of the exercise that are marked as **TODO**.\n",
    "A correctly completed **TODO** gives 2 bonus points. Partially correct answers give 1 bonus point.\n",
    "Some **TODO**s are inside a comment in a code block: Here, you should complete the line of code.\n",
    "Other **TODO**s are inside a text block: Here, you should write a few sentences to answer the question.\n",
    "\n",
    "**Important:** Some students were under the impression that you have to complete a TODO in a _single_ line of code. That is not the case, you can use as many lines as you need.\n",
    "\n",
    "**Submission deadline:** 31.01.2022, 23:59 Central European Time\n",
    "\n",
    "**Instructions for submission:** After completing the exercise, save a copy of the notebook as exercise10_multilinguality_MATRIKELNUMMER.ipynb, where MATRIKELNUMMER is your student ID number. Then upload the notebook to moodle (submission exercise sheet 10).\n",
    "\n",
    "In order to understand the code, it can be helpful to experiment a bit during development, e.g., to print vectors, matrices, and tensors or their shapes. But please remove these changes before submitting the notebook. If we cannot run your notebook, or if a print statement is congesting stdout too much, then we cannot grade it. \n",
    "\n",
    "To make the most of this exercise, you should try to read and understand the entire code, not just the parts that contain a **TODO**. If you have questions, write them down for the exercise, which will happen in the week after the submission deadline.\n",
    "\n",
    "**CUDA:** You can use a GPU for this exercise (on colab: Runtime -> Change Runtime Type -> GPU). This is not mandatory (nor particularly crucial for this exercise), but it may speed up the execution of the code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required libraries\n",
    "\n",
    "When working with or any fast-changing software library, you should be extra careful to fix the library versions when you begin your project, and not change versions while you're developing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "This exercise requires various data files (monolingual embeddings, translation dictionaries, training and evaluation data for cross-lingual transfer for sentiment classification) which have been zipped and need to be obtained from: \n",
    "\n",
    "https://tinyurl.com/2p9x4crt\n",
    "\n",
    "You need to place the content of the archive into a directory named **data** and place it in the same directory with this notebook (so that the files can be accessed by the code via a relative path \"data/file_name\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers==2.1.0\n",
    "!pip install numpy==1.20.0 # other numpy versions most likely also ok\n",
    "!pip install pandas==1.2.2\n",
    "!pip install scikit-learn==1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilingual word embeddings\n",
    "\n",
    "We will start by exploring multilingual word embedding spaces. We will start from monolingually-trained (that is, mutually unaligned) word vectors of several languages: English, German, Italian, and Croatian. \n",
    "\n",
    "Each monolingual embedding space is serialized in two files: a *pickled Python dictionary* (.vocab) that maps words to indices in the embedding matrix, and an embedding matrix (*a serialized 2D numpy array*, .vectors) that contains the actual embeddings. All embedding files should be in the **data** subdirectory, which should be in the same directory as this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will load pre-trained serialized monolingual vectors\n",
    "import pickle\n",
    "import numpy\n",
    "\n",
    "# Load (that is, \"unpickle\") the vocabularies for all languages \n",
    "vocab_en = #TODO: load the English vocabulary \n",
    "vectors_en = #TODO: load the English embedding vectors (2D numpy array) \n",
    "\n",
    "# let's see how many words we have in the vocabulary\n",
    "print(len(vocab_en))\n",
    "\n",
    "# let's see the dimensions of the embedding matrix\n",
    "print(vectors_en.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's see what the vector of some word looks like\n",
    "word = \"dog\"\n",
    "print(\"Index of \" + word + \": \" + str(vocab_en[word]))\n",
    "\n",
    "vector = #TODO: print the embedding vector of the above specified word \n",
    "#(currently \"dog\", but feel free to change to any other word)\n",
    "\n",
    "print(\"Vector of \" + word + \": \")\n",
    "print(vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us now load vectors and vocabularies of a few other languages\n",
    "\n",
    "# German\n",
    "vocab_de = #TODO: load the German vocabulary \n",
    "vectors_de = #TODO: load the German embeddings (2D numpy array) \n",
    "\n",
    "# Italian\n",
    "vocab_it = #TODO: load the Italian vocabulary \n",
    "vectors_it = #TODO: load the Italian embeddings (2D numpy array) \n",
    "\n",
    "# Croatian\n",
    "vocab_hr = #TODO: load the Croatian vocabulary  \n",
    "vectors_hr = #TODO: load the Croatian embeddings (2D numpy array) \n",
    "\n",
    "# let's see how many entries we have in vocabularies of languages\n",
    "print(\"DE\", len(vocab_de), vectors_de.shape)\n",
    "print(\"IT\", len(vocab_it), vectors_it.shape)\n",
    "print(\"HR\", len(vocab_hr), vectors_hr.shape)\n",
    "\n",
    "# TODO: What is the dimensionality of the embeddings (same for all languages :)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the vectors from individual monolingual embedding spaces **comparable**? **No, they are not**. Let's verify that. Let's compare vector similarities within language and across languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity is a common measure of similarity in vector space in NLP\n",
    "# we just define a function that compute the cosine of the angle between the two vectors\n",
    "# cosine similarity is a dot-product between the vectors normalized by the Euclidean (L2) norm of each vector\n",
    "def cosine_sim(vec1, vec2):\n",
    "    norm1 = numpy.linalg.norm(vec1)\n",
    "    norm2 = numpy.linalg.norm(vec2)\n",
    "    return numpy.dot(vec1, vec2) / (norm1 * norm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see some monolingual similarities\n",
    "\n",
    "language = \"de\" # play with different languages, \"en\", \"it\", \"hr\"\n",
    "\n",
    "# just a shortcut, so we don't have to change the variables with vectors/vocabularies, we merely change the \"language\" variable\n",
    "vectors = vectors_en if language == \"en\" else vectors_de if language == \"de\" else vectors_it if language == \"it\" else vectors_hr\n",
    "vocab = vocab_en if language == \"en\" else vocab_de if language == \"de\" else vocab_it if language == \"it\" else vocab_hr\n",
    "\n",
    "word1 = \"hund\" # play with different words\n",
    "word2 = \"katze\" # play with different words\n",
    "\n",
    "vector1 = #TODO: get the vector of the first word (word1) \n",
    "vector2 = #TODO: get the vector of the second word (word2)  \n",
    "\n",
    "sim = #TODO: compute the cosine similarity between the embedding vectors of the two words \n",
    "print(\"Similarity between \" + word1 + \" and\" + word2 + \": \" + str(sim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's put all embeddings and vocabularies into one dictionary\n",
    "# just for easy access\n",
    "emb_dict = {\"en\" : (vocab_en, vectors_en), \n",
    "            \"de\" : (vocab_de, vectors_de), \n",
    "            \"it\" : (vocab_it, vectors_it), \n",
    "            \"hr\" : (vocab_hr, vectors_hr)}\n",
    "\n",
    "# lets create a more general function for comparing similarities between words from any two langs\n",
    "def word_similarity(lang1, word1, lang2, word2):\n",
    "    vocab_1 = #TODO: get the vocabulary of the lang1\n",
    "    vectors_1 = #TODO: get the embeddings of the lang1 \n",
    "    \n",
    "    vocab_2 = #TODO: get the vocabulary of the lang2\n",
    "    vectors_2 = #TODO: get the embeddings of the lang2\n",
    "    \n",
    "    vector_word_1 = vectors_1[vocab_1[word1]]\n",
    "    vector_word_2 = vectors_2[vocab_2[word2]]\n",
    "    \n",
    "    return #TODO: return cosine similarity of vector_word_1 and vector_word_2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = word_similarity(\"de\", \"katze\", \"en\", \"cat\") #TODO: compute the similarity between the German word \"katze\" and English word \"cat\"\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the independently built monolingual word embedding spaces of different languages are **not semantically aligned**. We need to **align them**. \n",
    "\n",
    "- We will do this by computing a **projection matrix** that rotates and translates one embedding space with respect to the other! \n",
    "\n",
    "- How do we know what we need to align? We provide some number of word translation pairs! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "prefix_trans = \"/work/gglavas/data/word_embs/yacle/translations/freq_split/pairwise/\"\n",
    "\n",
    "word_pairs_de_en =  [(l.strip().split(\"\\t\")[0], l.strip().split(\"\\t\")[1]) for l in codecs.open(\"data/translations.5k.de-en.tsv\", \"r\", encoding = 'utf8', errors = 'replace').readlines()]\n",
    "word_pairs_it_en =  [(l.strip().split(\"\\t\")[1], l.strip().split(\"\\t\")[0]) for l in codecs.open(\"data/translations.5k.en-it.tsv\", \"r\", encoding = 'utf8', errors = 'replace').readlines()]\n",
    "word_pairs_hr_en =  [(l.strip().split(\"\\t\")[1], l.strip().split(\"\\t\")[0]) for l in codecs.open(\"data/translations.5k.en-hr.tsv\", \"r\", encoding = 'utf8', errors = 'replace').readlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_pairs_de_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's now create the matrices of aligned vectors of word translations, \n",
    "# given monolingual embeddings and word translation pairs\n",
    "\n",
    "def align_word_vectors(src_vecs, src_vocab, trg_vecs, trg_vocab, trans_pairs):\n",
    "    src_matrix =  []\n",
    "    trg_matrix =  []\n",
    "    \n",
    "    # for each pair of words in our translation pairs\n",
    "    for src_word, trg_word in trans_pairs:\n",
    "        # add the vector of the source language word to the source matrix\n",
    "        src_matrix.append(src_vecs[src_vocab[src_word.lower()]])\n",
    "        # add the vector of the corresponding (translation) target language word to the target matrix\n",
    "        trg_matrix.append(trg_vecs[trg_vocab[trg_word.lower()]])\n",
    "        \n",
    "    # return the row-aligned matrices (at the same index are vectors of mutual translations)\n",
    "    # from these matrices, we will compute the projection/alignment matrix using the Procrustes method\n",
    "    return numpy.array(src_matrix), numpy.array(trg_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_de_en, trg_de_en = #TODO: call the align_word_vectors to align embeddings for given translation pairs \n",
    "                       # between German (source) and English (target)  \n",
    "\n",
    "print(src_de_en.shape)\n",
    "print(trg_de_en.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's learn a projection matrix, given the aligned matrices of vectors of word translations\n",
    "# we will use the so-called Procrustes solution of the alignment problem (i.e., finding the optimal projection matrix)\n",
    "\n",
    "def get_projection_procrustes(src_mat, trg_mat):\n",
    "    product = #TODO: complete this line so the result is a correct projection matrix from source to target embedding space\n",
    "    U, S, V = numpy.linalg.svd(product)\n",
    "    \n",
    "    return numpy.matmul(U, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_mat_de_en = #TODO: obtain the projection matrix between German and English using the \n",
    "                 # translation-aligned embedding matrices \n",
    "\n",
    "#TODO: what is the shape of the projection matrix? Write the code that shows it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's now project the vectors of all the Italian words to the English vector space\n",
    "proj_vectors_de = numpy.matmul(vectors_de, proj_mat_de_en)\n",
    "\n",
    "# let's replace the original German vectors with the projected ones in the embeddings dictionary of all languages\n",
    "emb_dict[\"de\"] = (vocab_de, proj_vectors_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see now what the German-English similarities look like after projection of German embeddings to the English emb. space\n",
    "word_similarity(\"de\", \"katze\", \"en\", \"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's perform the mapping to the English embedding space for the other two languages as well\n",
    "\n",
    "## for Italian\n",
    "\n",
    "# aligning vectors of word translations\n",
    "src_it_en, trg_it_en = #TODO: align embeddings for translation pairs, Italian-English \n",
    "proj_mat_it_en = #TODO: Compute the projection matrix from Italian to English embedding space\n",
    "proj_vectors_it = #TODO: project the vectors of all Italian words to the English space, using the obtained projection matrix\n",
    "emb_dict[\"it\"] = #TODO: replace the original (unaligned) Italian vectors with the projected (aligned) ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_similarity(\"it\", \"gatto\", \"en\", \"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for Croatian\n",
    "\n",
    "src_hr_en, trg_hr_en = #TODO: align embeddings for translation pairs, Croatian-English  \n",
    "proj_mat_hr_en = #TODO: Compute the projection matrix from Croatian to English embedding space\n",
    "proj_vectors_hr = #TODO: project the vectors of all Croatian words to the English space, using the obtained projection matrix\n",
    "emb_dict[\"hr\"] = #TODO: replace the original (unaligned) Croatian vectors with the projected (aligned) ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_similarity(\"hr\", \"mačka\", \"en\", \"cat\"))\n",
    "\n",
    "# note that words of all four languages are now embedded in the same (originally English) embedding space\n",
    "# so we can semantically compare words between any two of these languages (not just X-EN)\n",
    "print(word_similarity(\"hr\", \"pas\", \"de\", \"hund\"))\n",
    "print(word_similarity(\"it\", \"gatto\", \"hr\", \"mačka\"))\n",
    "print(word_similarity(\"de\", \"flughafen\", \"it\", \"aeroporto\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained multilingual encoders!\n",
    "\n",
    "Pretrained multilingual transformers enable the comparison of meaning of longer units of text in different languages. To this end, we will use pretrained transformers specialized for encoding sentence-level semantics: SentenceTransformers (package sentence-transformers). More information: \n",
    "\n",
    "https://www.sbert.net/\n",
    "\n",
    "https://arxiv.org/pdf/1908.10084.pdf\n",
    "\n",
    "https://arxiv.org/pdf/2004.09813.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# loading the pretrained sentence encoder (concretely, the DistilmUSE model, distilled from multilingual USE)\n",
    "sent_encoder = SentenceTransformer('distiluse-base-multilingual-cased-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_en = \"Hello World\"\n",
    "sent_de = \"Hallo Welt\"\n",
    "sent_es = \"Hola mundo\"\n",
    "\n",
    "sentence_embeddings = #TODO: encode (that is, obtain the embeddings for) the above three sentences using the loaded sent_encoder\n",
    "print(sentence_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "sim_en_de = #TODO: Compute the cosine similarity between the embedding of the English sentence and the German sentence\n",
    "\n",
    "sim_de_es = #TODO: Compute the cosine similarity between the embedding of the Spanish sentence and the German sentence\n",
    "\n",
    "print(sim_en_de)\n",
    "print(sim_de_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-lingual transfer for downstream NLP tasks\n",
    "\n",
    "We will now see how to exploit a multilingual representation space (i.e., our multilingual sentence encoder) to train a model for a text classification task on annotated data in one language and then use that classification model to make predictions for texts from other languages.\n",
    "\n",
    "**Task**: sentiment classification of Amazon reviews\n",
    "\n",
    "**Annotated training data**: in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load the data using the *pandas* library\n",
    "# we have two files: the training dataset and testing dataset in English\n",
    "\n",
    "# importing the Python's Pandas library for data loading and manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Step #1: loading our annotated reviews\n",
    "train_data = pd.read_csv(\"data/labeled_train.txt\", delimiter = '\\t') # in our file, the values are actually TAB-separated\n",
    "eval_data = pd.read_csv(\"data/labeled_test.txt\", delimiter = '\\t')\n",
    "\n",
    "# let's see what our data actually looks like\n",
    "train_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task: predict the binary sentiment label (POS or NEG) from the encoding of the review text produced by the Sentence Transformer\n",
    "# as the classifier, we will use simple logistic regression model\n",
    "\n",
    "# filtering just the text from the pandas dataframe\n",
    "train_texts = list(train_data[\"content\"])\n",
    "\n",
    "# embedding all texts with the sentence encoder\n",
    "train_embeddings = #TODO: get the embeddings of the reviews (train_texts) with the sent_encoder\n",
    "\n",
    "# converting labels from \"POS\" and \"NEG\" into numeric labels (0 and 1) for classification\n",
    "train_labels = train_data[\"label\"].tolist()\n",
    "train_labels = [(1 if tl == \"POS\" else 0) for tl in train_labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have input (embeddings of train texts) and labels, we can use them to train a classifier\n",
    "# To this end, we will use a simple logistic regression classifier from scikit-learn\n",
    "\n",
    "# We import the LogisticRegression class from scikit-learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# we now train (\"fit\") the logistic regression classifier by providing the training input (SBERT embeddings of our training set texts) and \n",
    "# corresponding sentiment labels (0 or 1)\n",
    "\n",
    "classifier = LogisticRegression(C = 32, solver = 'lbfgs', max_iter = 1000)\n",
    "classifier.fit(train_embeddings, train_labels)\n",
    "\n",
    "# the result is a trained classifier, which we can examine more closely in the next steps and make predictions with\n",
    "print(classifier)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The classifier has now been trained. Let's predict the labels for the eval_set texts and see how accurate the classifier is\n",
    "# We first convert the evaluation texts into embeddings with our sentence_transformer and labels to binary {0,1} labels\n",
    "\n",
    "# filtering just the text from the pandas dataframe\n",
    "eval_texts = list(eval_data[\"content\"])\n",
    "eval_embeddings = #TODO: get the embeddings of the evaluation reviews (eval_texts) with the sent_encoder \n",
    "\n",
    "# converting labels from \"POS\" and \"NEG\" into numeric labels (0 and 1) for classification\n",
    "eval_labels = eval_data[\"label\"].tolist()\n",
    "eval_labels = [(1 if tl == \"POS\" else 0) for tl in eval_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we call the classifier to predict and score (function \"score\") the predictions on the evaluation dataset\n",
    "accuracy = classifier.score(eval_embeddings, eval_labels)\n",
    "print(\"Classification accuracy: \" + str(accuracy * 100) + \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see some predictions on individual texts\n",
    "\n",
    "# we are generating predictions as probability distributions over the two classes, for each text\n",
    "predict_probs = classifier.predict_proba(eval_embeddings)\n",
    "\n",
    "# for a text at \"index\", we'll display the text itself and the class probability distribution produced by the LR classifier\n",
    "# play by changing the \"index\"\n",
    "index = 100\n",
    "print(eval_texts[index])\n",
    "print(predict_probs[index])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and now, the **language transfer** (zero-shot)\n",
    "\n",
    "**Zero-shot** means that we are able to make predictions for texts in the target language without seeing any training instances in that language (i.e., our classifier was trained on English reviews). However, because the input features for the classifier are generated by a multilingual text encoder, we can make predictions for any language that our Sentence Transformer (sent_encoder) can encode! (and that's around 100 languages).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a few reviews in other languages:\n",
    "\n",
    "# \"This is a bad speaker, you can barely hear anything\"\n",
    "rev_de = \"Das ist ein ziemlich schlechter Lautsprecher, man kann kaum etwas hören.\"\n",
    "\n",
    "# \"The keyboard is light, easy to use and actually pretty beautiful. Totally worth the money!\"\n",
    "rev_it = \"La tastiera è leggera, facile da usare e in realtà piuttosto bella. Vale assolutamente i soldi!\" \n",
    "\n",
    "# This USB adapter is so average. Not bad, but I wouldn't give again that money for it. \n",
    "rev_hr = \"Ovaj USB adapter je onako, prosječan. Nije loš, ali ne bih ponovno dao te novce za njega.\"\n",
    "\n",
    "# This is by far the best USB charger I have ever had. My phone is charged in less than an hour, so amazing!\n",
    "rev_zh = \"这是迄今为止我所拥有的最好的USB充电器。我的手机在不到一小时内就充好电了，太神奇了!\"\n",
    "\n",
    "revs = [rev_de, rev_it, rev_hr, rev_zh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's encode the reviews with the sentence_transformer \n",
    "rev_embeddings = #TODO: embed the reviews above with the sent_encoder\n",
    "\n",
    "# now let's classify them and make predictions with our LR classifier\n",
    "predict_probs = classifier.predict_proba(rev_embeddings)\n",
    "\n",
    "# let's print each review and the predicted sentiment probabilities\n",
    "for i in range(len(revs)):\n",
    "    print(revs[i])\n",
    "    print(predict_probs[i])\n",
    "    pred_label = \"NEG\" if predict_probs[i][0] > predict_probs[i][1] else \"POS\"  \n",
    "    print(\"Predicted label: \" + pred_label)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: discuss the expected transfer performance for the above target languages. What could be the main factors\n",
    "# that determine the success of transfer of a classifier based on the pretrained multilingual encoder from the source language\n",
    "# (i.e., English) to a concrete target language?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
